---
title: My Notes
layout: post
author: Yan Wang
---

### Object Storage vs File Storage  
- flat structure vs hierarchy structure
- object storage used for large amount of data, archived data, cheap to store.
- distributed storage, replicated data redundancy. High availability.

### CAP theorem
- Avaiblability
  - Every request receives a (non-error) response, without the guarantee that it contains the most recent write.
- Consistency
  - Every read receives the most recent write or an error.
- partition tolerance 
  - The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.

### Spark
- Spark cache is lazy. 
```scala
DF1.cache()
DF1.count()
```
- We used the count above to eagerly cache the data (basically perform an action to force Spark to store it in memory), because caching itself is lazy—the data is cached only on the first time you run an action on the DataFrame.

- each executor will launch a jvm, so it's good we do not allocate more than one executor per machine.
- standalone cluster does not honor ```executor.num``` configuration, it used ```max.cores/executor.cores``` to figure out corresponding executor number.
- **ContextCleaner** — Spark Application Garbage Collector
ContextCleaner is a Spark service that is responsible for application-wide cleanup of shuffles, RDDs, broadcasts, accumulators and checkpointed RDDs that is aimed at reducing the memory requirements of long-running data-heavy Spark applications.
    - ContextCleaner runs on the driver. It is created and immediately started when SparkContext starts (and spark.cleaner.referenceTracking Spark property is enabled, which it is by default). It is stopped when SparkContext is stopped.
- Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.
![diagram](/assets/images/streaming-dstream-ops.png)
- Input DStreams are DStreams representing the stream of input data received from streaming sources. In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.

  - Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section). This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).  
     
- When running a Spark Streaming program locally, **do not** use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).
  - Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it.

- The spark-submit script provides the most straightforward way to submit a compiled Spark application to the cluster. For standalone clusters, Spark currently supports two deploy modes. In **client** mode, the driver is launched in the same process as the client that submits the application. In **cluster** mode, however, the driver is launched from one of the Worker processes inside the cluster, and the client process exits as soon as it fulfills its responsibility of submitting the application without waiting for the application to finish.
- Spark has three officially supported cluster managers:
    - Standalone mode
    - Hadoop YARN
    - Apache Mesos

- Spark Streaming + Kafka Integration two Approaches:
    1. Receiver Based Approach(old)
    1. Direct Approach(No receivers)
    - Each Kafka Partition mapped the RDD partition.
    - [https://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html](https://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html)
- How to use Spark-UI
    - [https://docs.google.com/presentation/d/134IbLANG6OyJQh-A5e74aLRHjOpqgdFzzvUZvh57EnU/edit#slide=id.g6f0b1b8094_0_44](https://docs.google.com/presentation/d/134IbLANG6OyJQh-A5e74aLRHjOpqgdFzzvUZvh57EnU/edit#slide=id.g6f0b1b8094_0_44)

### Kubernetes
  - aim to provide a platform for automating deployment, scaling and operations across clusters of hosts.

### Spark-submit command
```
./bin/spark-submit \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key<=<value> \
  --driver-memory <value>g \
  --executor-memory <value>g \
  --executor-cores <number of cores>  \
  --jars  <comma separated dependencies>
  --class <main-class> \
  <application-jar> \
  [application-arguments]
```

### Top Mistakes to Avoid in Streaming Applications (data summit 2023)
[https://www.youtube.com/watch?v=khhzniCyfP4](https://www.youtube.com/watch?v=khhzniCyfP4)
1. minPartitions config (virtual kafka partition will be created)
1. Data Ingestion - Rate limitation
  1. `maxOffsetsPerTrigger`



### Bash
`-z string`: True if the string is null (an empty string)

### ssh-config
Empty lines and lines starting with the hash sign (#) are comments. SSH ignores these lines and uses the first obtained values when searching through the config file. Place host-specific information near the beginning of the file and general default information near the end.
















